flux_path: "/mnt/data/wangzh/models/FLUX.1-Fill-dev/FLUX.1-Fill-dev" # "../ckpt/FLUX.1-Fill-dev"
dtype: "bfloat16"

model:
  union_cond_attn: true
  add_cond_attn: false
  latent_lora: false
  use_sep: false

train:
  batch_size: 1
  accumulate_grad_batches: 2
  dataloader_workers: 5
  save_interval: 500 # 1000
  sample_interval: 500 # 1000
  max_steps: -1
  gradient_checkpointing: false
  save_path: "/mnt/data/wangzh/kontext/checkpoints/multiple/runs" #"runs" # runs with multile

  condition_type: "edit"
  dataset:
    type: "edit_with_idlight"
    path: "parquet/*.parquet" #"../MagicBrush"
    csv_path: "/mnt/data/wangzh/kontext/data/multiple.csv" # "/home/wangzh/Techniques/ICEdit-My/new.csv"
    image_dir: "/mnt/data/wangzh/kontext/data/multiple"
    condition_size: 512
    target_size: 512
    image_size: 512
    padding: 8
    drop_text_prob: -1.0
    drop_image_prob: 0.1
    # specific_task: ["removal", "style", "attribute_modification", "env", "swap"]

  wandb:
    project: "OminiControl"

  lora_config:
    r: 32
    lora_alpha: 32
    init_lora_weights: "gaussian"
    target_modules: "(.*x_embedder|.*(?<!single_)transformer_blocks\\.[0-9]+\\.norm1\\.linear|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_k|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_q|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_v|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_out\\.0|.*(?<!single_)transformer_blocks\\.[0-9]+\\.ff\\.net\\.2|.*single_transformer_blocks\\.[0-9]+\\.norm\\.linear|.*single_transformer_blocks\\.[0-9]+\\.proj_mlp|.*single_transformer_blocks\\.[0-9]+\\.proj_out|.*single_transformer_blocks\\.[0-9]+\\.attn.to_k|.*single_transformer_blocks\\.[0-9]+\\.attn.to_q|.*single_transformer_blocks\\.[0-9]+\\.attn.to_v|.*single_transformer_blocks\\.[0-9]+\\.attn.to_out)"
    num_experts: 4
    expert_rank: 32
    expert_alpha: 32
    top_k: 2

  optimizer:
    type: "Prodigy"
    params:
      lr: 1
      use_bias_correction: true
      safeguard_warmup: true
      weight_decay: 0.01
